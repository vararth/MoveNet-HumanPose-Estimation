{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction \n\nIn this tutorial, we will make use of the next-generation Pose Detection model from Google Research, which can detect up to 17 keypoints in the human body. \n\n# MoveNet architecture in a nutshell \n\n## How does it work ? \n\nMoveNet uses heatmaps to accurately localize human keypoints. It's a **bottom-up** estimation model, which means that it first detects the human joints of all persons, and then assemble these joints into poses for each person.[[Source]](https://arxiv.org/pdf/1807.09972.pdf#:~:text=The%20top%2Ddown%20approaches%20first,full%20poses%20for%20all%20persons.)\n\n## Architecture (2 main components)\n\n- **Feature extractor** : A MobileNetV2 with an attached feature pyramid network. [Learn more about MobileNetV2](https://arxiv.org/pdf/1801.04381.pdf)\n- **A set of predictor heads** : attached to the feature extractor. They are responsible for predicting : \n - the geometric center of the instances (persons)\n - the full set of keypoints for a person\n - the location of all the keypoints\n - local offsets from each output feature map pixel to the precise sub-pixel location of each keypoint\n \n \n## [A deeper explanation on the MoveNet processing steps](https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html). Now, let's start coding ! \n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"# Computer vision/graphics library\nimport cv2\n\n# Gif writer\nimport imageio \n\n# Display libraries \nimport matplotlib.pyplot as plt \nfrom IPython.display import HTML, display\n\n# Calculations and Deep Learning library\nimport numpy as np \nimport tensorflow as tf \nimport tensorflow_hub as hub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-08T10:10:36.003433Z","iopub.execute_input":"2022-05-08T10:10:36.004125Z","iopub.status.idle":"2022-05-08T10:10:37.795445Z","shell.execute_reply.started":"2022-05-08T10:10:36.004067Z","shell.execute_reply":"2022-05-08T10:10:37.79472Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install -q git+https://github.com/tensorflow/docs","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T10:10:37.799487Z","iopub.execute_input":"2022-05-08T10:10:37.799752Z","iopub.status.idle":"2022-05-08T10:10:51.161607Z","shell.execute_reply.started":"2022-05-08T10:10:37.799718Z","shell.execute_reply":"2022-05-08T10:10:51.160806Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> We will use the \"embed\" module from TF Docs to embed the generated gif to our Notebook","metadata":{}},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"markdown","source":"## Map the bones (keypoint edges) to a matplotlib color name\n\n> ![Colors_index](https://raw.githubusercontent.com/Justsecret123/Human-pose-estimation/main/Screenshots/mpl_colors.PNG)","metadata":{}},{"cell_type":"code","source":"cyan = (255, 255, 0)\nmagenta = (255, 0, 255)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:10:51.164228Z","iopub.execute_input":"2022-05-08T10:10:51.16449Z","iopub.status.idle":"2022-05-08T10:10:51.169209Z","shell.execute_reply.started":"2022-05-08T10:10:51.164454Z","shell.execute_reply":"2022-05-08T10:10:51.167606Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EDGE_COLORS = {\n    (0, 1): magenta,\n    (0, 2): cyan,\n    (1, 3): magenta,\n    (2, 4): cyan,\n    (0, 5): magenta,\n    (0, 6): cyan,\n    (5, 7): magenta,\n    (7, 9): cyan,\n    (6, 8): magenta,\n    (8, 10): cyan,\n    (5, 6): magenta,\n    (5, 11): cyan,\n    (6, 12): magenta,\n    (11, 12): cyan,\n    (11, 13): magenta,\n    (13, 15): cyan,\n    (12, 14): magenta,\n    (14, 16): cyan\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:10:51.171823Z","iopub.execute_input":"2022-05-08T10:10:51.172395Z","iopub.status.idle":"2022-05-08T10:10:51.18036Z","shell.execute_reply.started":"2022-05-08T10:10:51.17236Z","shell.execute_reply":"2022-05-08T10:10:51.179714Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load the model from TF hub\n\n> *Notes* : MoveNet is offered with two variants, known as : \n> - **Lightning :** intended for latency-critical applications \n> - **Thunder :** for applications that require high accuracy \n\nWe will load the multipose *Lightning* model, which is able to detect mutliple people (up to 6 instances) in the image frame at the same time. ","metadata":{}},{"cell_type":"code","source":"model = hub.load(\"https://tfhub.dev/google/movenet/multipose/lightning/1\")\nmovenet = model.signatures[\"serving_default\"]","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-08T10:10:51.181803Z","iopub.execute_input":"2022-05-08T10:10:51.182236Z","iopub.status.idle":"2022-05-08T10:11:04.430233Z","shell.execute_reply.started":"2022-05-08T10:10:51.1822Z","shell.execute_reply":"2022-05-08T10:11:04.429459Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Adjust the input size \n\nAccording to [the official documentation](https://tfhub.dev/google/movenet/multipose/lightning/1) : \n\n1. The height/width are both multiple of 32.\n2. The height to width ratio is close (and enough) to cover the original image's aspect ratio.\n3. Make the larger side to be 256 (one should adjust this based on the speed/accuracy requirements). For example, a 720p image (i.e. 720x1280 (HxW)) should be resized and padded to 160x256 image.\n\nFollowing these rules, our input_gif would have been reshaped like this : \n- width : 461 ---> 256 (the larger side = 256). \n- height : 250 ---> 250 * (250/461) ~136. Since the rule 1 specifies the height to be a mutliple of 32, we'd have to round it to the closest one, which is 128. \n\n> *Note : 250/461 is the aspect ratio.  \n\nFor visualization purposes, we'll set the input_size to 256*256. ","metadata":{}},{"cell_type":"code","source":"#initial_width, initial_height = (461,250)\nWIDTH = HEIGHT = 256","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:11:04.431531Z","iopub.execute_input":"2022-05-08T10:11:04.431806Z","iopub.status.idle":"2022-05-08T10:11:04.436037Z","shell.execute_reply.started":"2022-05-08T10:11:04.431772Z","shell.execute_reply":"2022-05-08T10:11:04.435302Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"markdown","source":"## Download the test gif","metadata":{}},{"cell_type":"code","source":"! wget -O ngannou.gif https://raw.githubusercontent.com/Justsecret123/Human-pose-estimation/main/Test%20gifs/Ngannou_3.gif","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:17:14.327637Z","iopub.execute_input":"2022-05-08T10:17:14.328379Z","iopub.status.idle":"2022-05-08T10:17:15.471433Z","shell.execute_reply.started":"2022-05-08T10:17:14.328341Z","shell.execute_reply":"2022-05-08T10:17:15.470613Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define the loop\n\nSteps : loop through the results ---> Draw the keypoints ----> Draw the edges ","metadata":{}},{"cell_type":"code","source":"def loop(frame, keypoints, threshold=0.11):\n    \"\"\"\n    Main loop : Draws the keypoints and edges for each instance\n    \"\"\"\n    \n    # Loop through the results\n    for instance in keypoints: \n        # Draw the keypoints and get the denormalized coordinates\n        denormalized_coordinates = draw_keypoints(frame, instance, threshold)\n        # Draw the edges\n        draw_edges(denormalized_coordinates, frame, EDGE_COLORS, threshold)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:11:05.28044Z","iopub.execute_input":"2022-05-08T10:11:05.280798Z","iopub.status.idle":"2022-05-08T10:11:05.289327Z","shell.execute_reply.started":"2022-05-08T10:11:05.28075Z","shell.execute_reply":"2022-05-08T10:11:05.288186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Draw keypoints\n\n### Steps : \n- Get the parameters (shape): width, height, channel \n- Denormalize the output coordinates by mutliplying the keypoints with the with the parameters\n- Iterate through the denormalized keypoints and draw the circles where the confidence score is higher than the preset threshold","metadata":{}},{"cell_type":"code","source":"def draw_keypoints(frame, keypoints, threshold=0.11):\n    \"\"\"Draws the keypoints on a image frame\"\"\"\n    \n    # Denormalize the coordinates : multiply the normalized coordinates by the input_size(width,height)\n    denormalized_coordinates = np.squeeze(np.multiply(keypoints, [WIDTH,HEIGHT,1]))\n    #Iterate through the points\n    for keypoint in denormalized_coordinates:\n        # Unpack the keypoint values : y, x, confidence score\n        keypoint_y, keypoint_x, keypoint_confidence = keypoint\n        if keypoint_confidence > threshold:\n            \"\"\"\"\n            Draw the circle\n            Note : A thickness of -1 px will fill the circle shape by the specified color.\n            \"\"\"\n            cv2.circle(\n                img=frame, \n                center=(int(keypoint_x), int(keypoint_y)), \n                radius=4, \n                color=(255,0,0),\n                thickness=-1\n            )\n    return denormalized_coordinates","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:11:05.291175Z","iopub.execute_input":"2022-05-08T10:11:05.292045Z","iopub.status.idle":"2022-05-08T10:11:05.315361Z","shell.execute_reply.started":"2022-05-08T10:11:05.292001Z","shell.execute_reply":"2022-05-08T10:11:05.31413Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Draw the edges \n### Steps : \n- Iterate through the edges and draw the edges\n- Get the edge points and the associated dict value\n- Draw the lines where the confidence score is higher than the preset threshold","metadata":{}},{"cell_type":"code","source":"def draw_edges(denormalized_coordinates, frame, edges_colors, threshold=0.11):\n    \"\"\"\n    Draws the edges on a image frame\n    \"\"\"\n    \n    # Iterate through the edges \n    for edge, color in edges_colors.items():\n        # Get the dict value associated to the actual edge\n        p1, p2 = edge\n        # Get the points\n        y1, x1, confidence_1 = denormalized_coordinates[p1]\n        y2, x2, confidence_2 = denormalized_coordinates[p2]\n        # Draw the line from point 1 to point 2, the confidence > threshold\n        if (confidence_1 > threshold) & (confidence_2 > threshold):      \n            cv2.line(\n                img=frame, \n                pt1=(int(x1), int(y1)),\n                pt2=(int(x2), int(y2)), \n                color=color, \n                thickness=2, \n                lineType=cv2.LINE_AA # Gives anti-aliased (smoothed) line which looks great for curves\n            )","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:11:05.318061Z","iopub.execute_input":"2022-05-08T10:11:05.319731Z","iopub.status.idle":"2022-05-08T10:11:05.328648Z","shell.execute_reply.started":"2022-05-08T10:11:05.319671Z","shell.execute_reply":"2022-05-08T10:11:05.327495Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Progress bar","metadata":{}},{"cell_type":"code","source":"def progress(value, max=100):\n    \"\"\"\n    Returns an HTML progress bar with a certain value. Used within each step\n    \"\"\"\n    \n    \n    return HTML(\"\"\"\n      <progress\n          value='{value}'\n          max='{max}',\n          style='width: 100%'\n      >\n          {value}\n      </progress>\n  \"\"\".format(value=value,\n                max=max))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:11:05.329993Z","iopub.execute_input":"2022-05-08T10:11:05.33077Z","iopub.status.idle":"2022-05-08T10:11:05.344912Z","shell.execute_reply.started":"2022-05-08T10:11:05.330726Z","shell.execute_reply":"2022-05-08T10:11:05.343975Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Capture and process each frame","metadata":{}},{"cell_type":"markdown","source":"### Load the gif","metadata":{}},{"cell_type":"code","source":"def load_gif():\n    \"\"\"\n    Loads the gif and return its details\n    \"\"\"\n    \n    # Load the gif\n    gif = cv2.VideoCapture(\"./ngannou.gif\")\n    # Get the frame count\n    frame_count = int(gif.get(cv2.CAP_PROP_FRAME_COUNT))\n    # Display parameter\n    print(f\"Frame count: {frame_count}\")\n    \n    \"\"\"\"\"\n    Initialize the video writer \n    We'll append each frame and its drawing to a vector, then stack all the frames to obtain a sequence (video). \n    \"\"\"\n    output_frames = []\n    \n    # Get the initial shape (width, height)\n    initial_shape = []\n    initial_shape.append(int(gif.get(cv2.CAP_PROP_FRAME_WIDTH)))\n    initial_shape.append(int(gif.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n    \n    return gif, frame_count, output_frames, initial_shape","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:11:05.36132Z","iopub.execute_input":"2022-05-08T10:11:05.361782Z","iopub.status.idle":"2022-05-08T10:11:05.377122Z","shell.execute_reply.started":"2022-05-08T10:11:05.361733Z","shell.execute_reply":"2022-05-08T10:11:05.376368Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Run inference","metadata":{}},{"cell_type":"code","source":"def run_inference():\n    \"\"\"\n    Runs inferences then starts the main loop for each frame\n    \"\"\"\n    \n    # Load the gif\n    gif, frame_count, output_frames, initial_shape = load_gif()\n    # Set the progress bar to 0. It ranges from the first to the last frame\n    bar = display(progress(0, frame_count-1), display_id=True)\n    \n    # Loop while the gif is opened\n    while gif.isOpened():\n        \n        # Capture the frame\n        ret, frame = gif.read()\n        \n        # Exit if the frame is empty\n        if frame is None: \n            break\n        \n        # Retrieve the frame index\n        current_index = gif.get(cv2.CAP_PROP_POS_FRAMES)\n        \n        # Copy the frame\n        image = frame.copy()\n        image = cv2.resize(image, (WIDTH,HEIGHT))\n        # Resize to the target shape and cast to an int32 vector\n        input_image = tf.cast(tf.image.resize_with_pad(image, WIDTH, HEIGHT), dtype=tf.int32)\n        # Create a batch (input tensor)\n        input_image = tf.expand_dims(input_image, axis=0)\n\n        # Perform inference\n        results = movenet(input_image)\n        \"\"\"\n        Output shape :  [1, 6, 56] ---> (batch size), (instances), (xy keypoints coordinates and score from [0:50] \n        and [ymin, xmin, ymax, xmax, score] for the remaining elements)\n        First, let's resize it to a more convenient shape, following this logic : \n        - First channel ---> each instance\n        - Second channel ---> 17 keypoints for each instance\n        - The 51st values of the last channel ----> the confidence score.\n        Thus, the Tensor is reshaped without losing important information. \n        \"\"\"\n        \n        keypoints = results[\"output_0\"].numpy()[:,:,:51].reshape((6,17,3))\n\n        # Loop through the results\n        loop(image, keypoints, threshold=0.11)\n        \n        # Get the output frame : reshape to the original size\n        frame_rgb = cv2.cvtColor(\n            cv2.resize(\n                image,(initial_shape[0], initial_shape[1]), \n                interpolation=cv2.INTER_LANCZOS4\n            ), \n            cv2.COLOR_BGR2RGB # OpenCV processes BGR images instead of RGB\n        ) \n        \n        # Add the drawings to the output frames\n        output_frames.append(frame_rgb)\n        \n        # Update the progress bar\n        bar.update(progress(current_index, frame_count-1))\n    \n    # Release the object\n    gif.release()\n    \n    print(\"Completed !\")\n    \n    return output_frames","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:11:05.38053Z","iopub.execute_input":"2022-05-08T10:11:05.381245Z","iopub.status.idle":"2022-05-08T10:11:05.397033Z","shell.execute_reply.started":"2022-05-08T10:11:05.381199Z","shell.execute_reply":"2022-05-08T10:11:05.396302Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_frames = run_inference()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:17:21.750829Z","iopub.execute_input":"2022-05-08T10:17:21.751108Z","iopub.status.idle":"2022-05-08T10:17:24.460454Z","shell.execute_reply.started":"2022-05-08T10:17:21.751076Z","shell.execute_reply":"2022-05-08T10:17:24.459629Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Display the results","metadata":{}},{"cell_type":"code","source":"from tensorflow_docs.vis import embed","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:17:24.462221Z","iopub.execute_input":"2022-05-08T10:17:24.462974Z","iopub.status.idle":"2022-05-08T10:17:24.467192Z","shell.execute_reply.started":"2022-05-08T10:17:24.462931Z","shell.execute_reply":"2022-05-08T10:17:24.466124Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Stack the output frames horizontally to compose a sequence\noutput = np.stack(output_frames, axis=0) \n# Write the sequence to a gif\nimageio.mimsave(\"./animation.gif\", output, fps=15) \n# Embed the output to the notebook\nembed.embed_file(\"./animation.gif\") ","metadata":{"execution":{"iopub.status.busy":"2022-05-08T10:17:24.469431Z","iopub.execute_input":"2022-05-08T10:17:24.469782Z","iopub.status.idle":"2022-05-08T10:17:36.506512Z","shell.execute_reply.started":"2022-05-08T10:17:24.469712Z","shell.execute_reply":"2022-05-08T10:17:36.505111Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Thank you for your time ! :-)\n\nAcknowledgements : \n- TF tutorials \n\nIbrahim SEROUIS, 2022","metadata":{}}]}